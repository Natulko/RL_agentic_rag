from infer import run_ollama

def reward_function(method, node_answer, true_answer=None, root_query=None):
    """
    Evaluate value of the given node based on the specified method.
    @param method: The method to use for evaluation: "llm", "em", "em_relaxed", jaccard", "accuracy", "f1"
    @param node_answer: The answer generated by the model
    @param true_answer: The true answer to compare against
    @param root_query: The original query to measure relevance of the answer
    """
    if true_answer is None:
        return 0.5 # default score if true answer is not provided

    node_answer_normalized = node_answer.lower().strip()
    true_answer_normalized = true_answer.lower().strip()

    if method == "llm":
        return reward_llm(node_answer_normalized, true_answer_normalized, root_query)
    elif method == "em":
        return reward_em(node_answer_normalized, true_answer_normalized)
    elif method == "em_relaxed":
        return reward_em_relaxed(node_answer_normalized, true_answer_normalized)
    elif method == "jaccard":
        return reward_jaccard_similarity(node_answer_normalized, true_answer_normalized)
    elif method == "accuracy":
        return reward_accuracy(node_answer_normalized, true_answer_normalized)
    elif method == "f1":
        return reward_f1(node_answer_normalized, true_answer_normalized)
    else:
        raise ValueError(f"Unknown method: {method}")

def reward_llm(node_answer, true_answer, root_query=None):
    """
    Evaluate value of the given node based on response from LLM.
    """
    prompt = f"""On a scale from 0.0 to 1.0, rate how relevant and helpful the following answer is to the question. Give only the numerical score.
Question: {root_query}
Given answer: {node_answer}
Score (0.0-1.0): """
    response = run_ollama(prompt, model)

    try:
        score = float(response.split("Score:")[-1].strip())
        if 0 <= score <= 1:
            return score
        else:
            return 0.5 # default score if not properly parsed
    except:
        return 0.5 # default score if parsing fails

def reward_em(node_answer, true_answer):
    """
    Evaluate value of the given node based on Exact String Matching.
    """ 
    if node_answer == true_answer:
        return 1.0
    else:
        return 0

def reward_em_relaxed(node_answer, true_answer):
    """
    Evaluate value of the given node based on Exact String Matching.
    """ 
    if node_answer == true_answer:
        return 1.0
    elif node_answer in true_answer or true_answer in node_answer:
        return 0.8
    else:
        return 0

def reward_jaccard_similarity(node_answer, true_answer):
    """
    Evaluate value of the given node based on Jaccard Similarity.
    """ 
    node_tokens = set(node_answer.split())
    true_tokens = set(true_answer.split())

    intersec = len(node_tokens.intersection(true_tokens))
    union = len(node_tokens.union(true_tokens))
    if union == 0:
        return 0
    else:
        return intersec / union

def reward_accuracy(node_answer, true_answer):
    """
    Evaluate value of the given node based on Accuracy.
    """ 
    if true_answer in node_answer:
        return 1.0
    else:
        return 0

def reward_f1(node_answer, true_answer):
    """
    Evaluate value of the given node based on F1 score.
    """ 
    node_tokens = set(node_answer.split())
    true_tokens = set(true_answer.split())

    prec = len(node_tokens.intersection(true_tokens)) / len(node_tokens) if len(node_tokens) > 0 else 0
    rec = len(node_tokens.intersection(true_tokens)) / len(true_tokens) if len(true_tokens) > 0 else 0
    if prec + rec == 0:
        return 0
    else:
        f1_score = 2 * (prec * rec) / (prec + rec)
        return f1_score